{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cRfmdWqJjKV",
        "outputId": "0c904051-2a34-4e50-e141-1cd452d0adcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjftsDeuPPK3",
        "outputId": "766c2f08-8d16-4487-dced-11bafc6449c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42-lJ1u9IHT6",
        "outputId": "25ae714a-34d1-4372-cecc-a8c042f5d583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.2-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.2\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece \n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KE_TpNaSZQ5n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jrlMx3Q0Kw6y"
      },
      "outputs": [],
      "source": [
        "# !unzip RuCoS.zip -d  /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpKx43Iq6znw"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DNY9HLqwg7S_"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os, re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "O9rzxF8MhQFf"
      },
      "outputs": [],
      "source": [
        "lst = ['\"', \"'\", '«', '»', '/', ')', '(']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aiwe1WT-g99Y"
      },
      "outputs": [],
      "source": [
        "def prepeare_text(text, lst):\n",
        "  for i in lst:\n",
        "    text = text.replace(i, ' ')\n",
        "  text = ' '.join(text.split())\n",
        "  text = text.replace(\".\", \". \")\n",
        "  text = text.replace(\",\", \", \")\n",
        "  text = text.replace(\"!\", \"! \")\n",
        "  text = text.replace(\"?\", \"? \")\n",
        "\n",
        "  return text\n",
        "\n",
        "def get_X_y_for_bert(data_json_file):\n",
        "    idx, X_a, X_b, y = [], [], [], []\n",
        "    with open(data_json_file, 'r') as json_file:\n",
        "        json_list = list(json_file)\n",
        "        for json_str in json_list:\n",
        "            item = json.loads(json_str)\n",
        "            text = item['passage']['text']\n",
        "            text1 = item['passage']['text']\n",
        "            new_text = []\n",
        "            for i in text.split():\n",
        "                if '@' not in i:\n",
        "                    new_text.append(i)\n",
        "            text = ' '.join(new_text)\n",
        "            text = prepeare_text(text, lst)\n",
        "            entities = item['passage']['entities']\n",
        "            for query in item['qas']:\n",
        "                ques = query['query']\n",
        "                text = prepeare_text(ques, lst)\n",
        "                for i in range(len(entities)):\n",
        "                    entities[i]['text'] = text1[entities[i]['start']: entities[i]['end']]\n",
        "                for i in entities:\n",
        "                    if i in query['answers']:\n",
        "                        y.append(1)\n",
        "                        X_a.append(text)\n",
        "                        X_b.append(ques.replace('@placeholder', i['text']))\n",
        "                        idx.append(0)\n",
        "                    else:\n",
        "                        y.append(0)\n",
        "                        X_a.append(text)\n",
        "                        X_b.append(ques.replace('@placeholder', i['text']))\n",
        "                        idx.append(0)\n",
        "                    if len((X_a[-1] + ' ' + X_b[-1]).split()) >= 300:\n",
        "                        del y[-1]\n",
        "                        del X_a[-1]\n",
        "                        del X_b[-1]\n",
        "                        del idx[-1]\n",
        "    return idx, X_a, X_b,  y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dfevOCWuhGdu"
      },
      "outputs": [],
      "source": [
        "idx_train, X_train_a, X_train_b, y_train = get_X_y_for_bert('/content/drive/MyDrive/NTI/train.jsonl')\n",
        "idx_val, X_test_a, X_test_b, y_test = get_X_y_for_bert('/content/drive/MyDrive/NTI/val.jsonl')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_a"
      ],
      "metadata": {
        "id": "_Qz6oCBeZW6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNsaM1XbhVgb",
        "outputId": "fcdf7f5e-94e0-4964-d62d-ba3bd163809b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id                                          sentence1  \\\n",
            "0   0  Кроме того,  серьезным вызовом для России стан...   \n",
            "1   0  Кроме того,  серьезным вызовом для России стан...   \n",
            "2   0  Кроме того,  серьезным вызовом для России стан...   \n",
            "3   0  Кроме того,  серьезным вызовом для России стан...   \n",
            "4   0  Кроме того,  серьезным вызовом для России стан...   \n",
            "\n",
            "                                           sentence2  label  \n",
            "0  Кроме того, серьезным вызовом для России стано...      0  \n",
            "1  Кроме того, серьезным вызовом для России стано...      0  \n",
            "2  Кроме того, серьезным вызовом для России стано...      0  \n",
            "3  Кроме того, серьезным вызовом для России стано...      0  \n",
            "4  Кроме того, серьезным вызовом для России стано...      0  \n",
            "   id                                          sentence1  \\\n",
            "0   0  В него вошли @placeholder,  Россия,  Украина и...   \n",
            "1   0  В него вошли @placeholder,  Россия,  Украина и...   \n",
            "2   0  В него вошли @placeholder,  Россия,  Украина и...   \n",
            "3   0  В него вошли @placeholder,  Россия,  Украина и...   \n",
            "4   0  В него вошли @placeholder,  Россия,  Украина и...   \n",
            "\n",
            "                                           sentence2  label  \n",
            "0       В него вошли ООН, Россия, Украина и Франция.      0  \n",
            "1  В него вошли Донбасса, Россия, Украина и Франция.      0  \n",
            "2      В него вошли МИДа, Россия, Украина и Франция.      0  \n",
            "3   В него вошли Берлине, Россия, Украина и Франция.      0  \n",
            "4  В него вошли Германии, Россия, Украина и Франция.      1  \n"
          ]
        }
      ],
      "source": [
        "df_train = pd.DataFrame({\n",
        "    'id': idx_train,\n",
        "    'sentence1': X_train_a,\n",
        "    'sentence2': X_train_b,\n",
        "    'label':y_train\n",
        "})\n",
        "\n",
        "print(df_train.head())\n",
        "\n",
        "df_val = pd.DataFrame({\n",
        "    'id': idx_val,\n",
        "    'sentence1': X_test_a,\n",
        "    'sentence2': X_test_b,\n",
        "    'label': y_test\n",
        "})\n",
        "\n",
        "print(df_val.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "f-GMisL9Kw61"
      },
      "outputs": [],
      "source": [
        "df_train['sentence1'] = df_train['sentence1'].str.lower()\n",
        "df_train['sentence2'] = df_train['sentence2'].str.lower()\n",
        "df_val['sentence2']   = df_val['sentence2'].str.lower()\n",
        "df_val['sentence1']   = df_val['sentence1'].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZyX0SaS_MkND"
      },
      "outputs": [],
      "source": [
        "df_train = df_train.drop_duplicates(subset=['sentence2'])\n",
        "df_val = df_val.drop_duplicates(subset=['sentence2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUh6CicTJqBW",
        "outputId": "26518ba4-2d3f-46bf-952d-1de80efc6420"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id                                          sentence1  \\\n",
            "0   0  кроме того,  серьезным вызовом для россии стан...   \n",
            "1   0  кроме того,  серьезным вызовом для россии стан...   \n",
            "2   0  кроме того,  серьезным вызовом для россии стан...   \n",
            "3   0  кроме того,  серьезным вызовом для россии стан...   \n",
            "4   0  кроме того,  серьезным вызовом для россии стан...   \n",
            "\n",
            "                                           sentence2  label  \n",
            "0  кроме того, серьезным вызовом для россии стано...      0  \n",
            "1  кроме того, серьезным вызовом для россии стано...      0  \n",
            "2  кроме того, серьезным вызовом для россии стано...      0  \n",
            "3  кроме того, серьезным вызовом для россии стано...      0  \n",
            "4  кроме того, серьезным вызовом для россии стано...      0  \n"
          ]
        }
      ],
      "source": [
        "df_train = pd.DataFrame({\n",
        "    'id': df_train['id'].values.tolist(),\n",
        "    'sentence1': df_train['sentence1'].values.tolist(),\n",
        "    'sentence2': df_train['sentence2'].values.tolist(),\n",
        "    'label':df_train['label'].values.tolist()\n",
        "})\n",
        "\n",
        "print(df_train.head())\n",
        "\n",
        "df_val = pd.DataFrame({\n",
        "    'id': df_val['id'].values.tolist(),\n",
        "    'sentence1': df_val['sentence1'].values.tolist(),\n",
        "    'sentence2': df_val['sentence2'].values.tolist(),\n",
        "    'label': df_val['label'].values.tolist()\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWJfh6DV7CB5"
      },
      "source": [
        "## Classes and functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Tc1GQh7yEm4C"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, maxlen, with_labels=True, bert_model = \"\"):\n",
        "\n",
        "        self.data = data \n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(bert_model)  \n",
        "\n",
        "        self.maxlen = maxlen\n",
        "        self.with_labels = with_labels \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sent1 = str(self.data.loc[index, 'sentence1'])\n",
        "        sent2 = str(self.data.loc[index, 'sentence2'])\n",
        "        encoded_pair = self.tokenizer(sent1, sent2, \n",
        "                                      padding='max_length', \n",
        "                                      truncation=True, \n",
        "                                      max_length=self.maxlen,  \n",
        "                                      return_tensors='pt', \n",
        "                                      return_token_type_ids=True) \n",
        "        \n",
        "        token_ids = encoded_pair['input_ids'].squeeze(0) \n",
        "        attn_masks = encoded_pair['attention_mask'].squeeze(0)  \n",
        "        token_type_ids = encoded_pair['token_type_ids'].squeeze(0)  \n",
        "\n",
        "        if self.with_labels:  \n",
        "            label = self.data.loc[index, 'label']\n",
        "            idx = self.data.loc[index, 'id']\n",
        "            return token_ids, attn_masks, token_type_ids, label, idx  \n",
        "        else:\n",
        "            return token_ids, attn_masks, token_type_ids, idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hm0lAXvTZChm"
      },
      "outputs": [],
      "source": [
        "class SentencePairClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, bert_model=\"DeepPavlov/rubert-base-cased-sentence\", freeze_bert=False):\n",
        "        super(SentencePairClassifier, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(bert_model)\n",
        "        if freeze_bert:\n",
        "            for p in self.bert_layer.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        D_in, H, D_out = 1024, 512, 1\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "    @autocast()\n",
        "    def forward(self, input_ids, attn_masks, token_type_ids):\n",
        "     \n",
        "\n",
        "        outputs = self.bert(input_ids, attn_masks, token_type_ids)\n",
        "        \n",
        "  \n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "        \n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wottYF3MUS5A"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5SrSNNYTjwe8"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "X6ptl87LgHRf"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MJmXi_NNWHkx"
      },
      "outputs": [],
      "source": [
        "def evaluate_loss(net, device, criterion, dataloader):\n",
        "    net.eval()\n",
        "\n",
        "    mean_loss = 0\n",
        "    count = 0\n",
        "    mean_acc = 0\n",
        "\n",
        "    fin_targets=[]\n",
        "    fin_outputs=[]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for it, (seq, attn_masks, token_type_ids, labels, _) in enumerate(tqdm(dataloader)):\n",
        "            seq, attn_masks, token_type_ids, labels = \\\n",
        "                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\n",
        "            logits = net(seq, attn_masks, token_type_ids)\n",
        "            fin_targets.extend(labels.float().cpu().detach().numpy().tolist())\n",
        "            fin_outputs.extend(torch.sigmoid(logits.float()).cpu().detach().numpy().tolist())\n",
        "            count += 1\n",
        "    \n",
        "    lst_out = []\n",
        "    for i in range(len(fin_targets)):\n",
        "      ans = 0\n",
        "      if fin_outputs[i][0] >= 0.5:\n",
        "        ans = 1\n",
        "      lst_out.append(ans)\n",
        "      fin_targets[i] = int(fin_targets[i])\n",
        "\n",
        "    return f1_score(fin_targets, lst_out, average='binary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl-rhuWsrg01",
        "outputId": "28bd86a2-907e-4aeb-f1da-69efe68f6611"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creation of the models' folder...\n",
            "mkdir: cannot create directory ‘/content/drive/MyDrive/models’: File exists\n"
          ]
        }
      ],
      "source": [
        "print(\"Creation of the models' folder...\")\n",
        "!mkdir /content/drive/MyDrive/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "I-o6KyaFkU5u"
      },
      "outputs": [],
      "source": [
        "def train_bert(net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate, path_to_save):\n",
        "\n",
        "    best_loss = -1\n",
        "    best_ep = 1\n",
        "    nb_iterations = len(train_loader)\n",
        "    print_every = nb_iterations // 5 \n",
        "    iters = []\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for ep in range(epochs):\n",
        "\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for it, (seq, attn_masks, token_type_ids, labels, _) in enumerate(tqdm(train_loader)):\n",
        "\n",
        "            seq, attn_masks, token_type_ids, labels = \\\n",
        "                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\n",
        "    \n",
        "            with autocast():\n",
        "                logits = net(seq, attn_masks, token_type_ids)\n",
        "\n",
        "                loss = criterion(logits.squeeze(-1), labels.float())\n",
        "                loss = loss / iters_to_accumulate \n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if (it + 1) % iters_to_accumulate == 0:\n",
        "                scaler.step(opti)\n",
        "                scaler.update()\n",
        "                lr_scheduler.step()\n",
        "                opti.zero_grad()\n",
        "\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if (it + 1) % print_every == 0:  # Print training loss information\n",
        "                print()\n",
        "                print(\"Iteration {}/{} of epoch {} complete. Loss : {} \"\n",
        "                      .format(it+1, nb_iterations, ep+1, running_loss / print_every))\n",
        "\n",
        "                running_loss = 0.0\n",
        "\n",
        "\n",
        "        val_acc = evaluate_loss(net, device, criterion, val_loader)  # Compute validation loss\n",
        "        print()\n",
        "        print(\"Epoch {} complete! Validation ACC : {}\".format(ep+1, val_acc))\n",
        "\n",
        "        if val_acc > best_loss:\n",
        "            print(\"Best validation ACC improved from {} to {}\".format(best_loss, val_acc))\n",
        "            print()\n",
        "            net_copy = deepcopy(net)  # save a copy of the model\n",
        "            best_loss = val_acc\n",
        "            best_ep = ep + 1\n",
        "\n",
        "            path_to_model='{}/{}_val_loss_{}_ep_{}.pt'.format(path_to_save, bert_model.replace('/', 'r'), round(best_loss, 3), best_ep)\n",
        "            torch.save(net_copy.state_dict(), path_to_model)\n",
        "            print(\"The model has been saved in {}\".format(path_to_model))\n",
        "\n",
        "    del loss\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFy9kQ2-SvQ2"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "RXe-tdXjJuV3"
      },
      "outputs": [],
      "source": [
        "#ЗАПУСК"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from multipledispatch import dispatch\n",
        "\n",
        "class Forest:\n",
        "  @dispatch(str, int)\n",
        "  def __init__(self, bert_model, count_models):\n",
        "    self.bert_model = bert_model\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    self.models = [SentencePairClassifier(self.bert_model, freeze_bert=freeze_bert).to(device) for i in range(count_models)]\n",
        "\n",
        "  @dispatch(str, list)\n",
        "  def __init__(self, bert_model, model_paths_):\n",
        "    self.bert_model = bert_model\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.models = [SentencePairClassifier(self.bert_model, freeze_bert=freeze_bert).to(device) for i in range(len(model_paths_))]\n",
        "    \n",
        "    for net, path_to_model in zip(self.models, model_paths_):\n",
        "      net.load_state_dict(torch.load(path_to_model))\n",
        "\n",
        "\n",
        "  def train(self, df_train, df_val,  freeze_bert = False, maxlen = 315, bs = 32, iters_to_accumulate = 2, lr = 0.000005, epochs = 3):\n",
        "\n",
        "    for index, net in enumerate(self.models):\n",
        "      train = df_train.sample(int(df_train.shape[0] * 0.5)).reset_index(drop = True)\n",
        "      print(\"Reading training data...\")\n",
        "      train_set = CustomDataset(train, maxlen, True, bert_model)\n",
        "      print(\"Reading validation data...\")\n",
        "      val_set      = CustomDataset(df_val, maxlen, True, bert_model)\n",
        "      train_loader = DataLoader(train_set, batch_size=bs, num_workers=5)\n",
        "      val_loader   = DataLoader(val_set, batch_size=bs, num_workers=5)\n",
        "\n",
        "\n",
        "      criterion = nn.BCEWithLogitsLoss()\n",
        "      opti = AdamW(net.parameters(), lr=lr, weight_decay=1e-2)\n",
        "      num_warmup_steps = 0 # The number of steps for the warmup phase.\n",
        "      num_training_steps = epochs * len(train_loader)  # The total number of training steps\n",
        "      t_total = (len(train_loader) // iters_to_accumulate) * epochs  # Necessary to take into account Gradient accumulation\n",
        "      lr_scheduler = get_linear_schedule_with_warmup(optimizer=opti, num_warmup_steps=num_warmup_steps, num_training_steps=t_total)\n",
        "\n",
        "      path = \"/content/drive/MyDrive/models/model{}/\".format(2)\n",
        "\n",
        "      if not os.path.exists(path):\n",
        "          os.makedirs(path)\n",
        "      \n",
        "      train_bert(net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate, path)\n",
        "  \n",
        "  def get_probs_from_logits(self, logits):\n",
        "\n",
        "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "    return probs.detach().cpu().numpy()\n",
        "\n",
        "  def predict(self, net, device, data, with_labels=True, result_file=\"results/output.txt\"):\n",
        "\n",
        "    net.eval()\n",
        "    probs_all = np.zeros(data.shape)\n",
        "    idx_all = []\n",
        "\n",
        "    dataset = CustomDataset(data, maxlen, self.bert_model)\n",
        "    dataloader = DataLoader(dataset, batch_size=bs, num_workers=5)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      for index, net in enumerate(self.models):\n",
        "          for seq, attn_masks, token_type_ids, labels, idx in tqdm(dataloader):\n",
        "              seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\n",
        "              logits = net(seq, attn_masks, token_type_ids)\n",
        "              probs = [i[0] for i in torch.sigmoid(logits.float()).cpu().detach().numpy().tolist()]\n",
        "              \n",
        "              idx_all += idx\n",
        "              probs_all[index] += probs\n",
        "    \n",
        "    \n",
        "    return idx_all, probs_all.mean(axis=1)\n"
      ],
      "metadata": {
        "id": "GYww2e2KuOYj"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "b6bzDp4FreS6"
      },
      "outputs": [],
      "source": [
        "bert_model = \"sismetanin/xlm_roberta_large-ru-sentiment-rureviews\"\n",
        "freeze_bert = False  \n",
        "maxlen = 315 \n",
        "bs = 2\n",
        "iters_to_accumulate = 2  \n",
        "lr = 0.000005  \n",
        "epochs = 4 "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "forest = Forest(bert_model, 3)\n",
        "forest.train(df_train, df_val)"
      ],
      "metadata": {
        "id": "1H2iwJ5B14nD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYW9hiE5J2WE"
      },
      "outputs": [],
      "source": [
        "#С СОХРАНЕНИЯ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDBtVu7JSbUK"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpDoq40awF7p"
      },
      "outputs": [],
      "source": [
        "def get_X_y_for_bert_test(data_json_file):\n",
        "    idx, X_a, X_b,  y = [], [], [], []\n",
        "    dct = {}\n",
        "    with open(data_json_file, 'r') as json_file:\n",
        "        json_list = list(json_file)\n",
        "        for json_str in json_list:\n",
        "            item = json.loads(json_str)\n",
        "            text = item['passage']['text']\n",
        "            text1 = item['passage']['text']\n",
        "            new_text = []\n",
        "            for i in text.split():\n",
        "                if '@' not in i:\n",
        "                    new_text.append(i)\n",
        "            text = ' '.join(new_text)\n",
        "            for i in lst:\n",
        "                text = text.replace(i, ' ')\n",
        "            text = ' '.join(text.split())\n",
        "            entities = item['passage']['entities']\n",
        "            if len(item['qas']) > 1:\n",
        "                print(1)\n",
        "            for query in item['qas']:\n",
        "                ques = query['query']\n",
        "                for i in lst:\n",
        "                    ques = ques.replace(i, ' ')\n",
        "                ques = ' '.join(ques.split())\n",
        "                for i in range(len(entities)):\n",
        "                    entities[i]['text'] = text1[entities[i]['start']: entities[i]['end']]\n",
        "                c = 0\n",
        "                for i in entities:\n",
        "                    c += 1\n",
        "                    y.append(1)\n",
        "                    X_a.append(text)\n",
        "                    X_b.append(ques.replace('@placeholder', i['text']))\n",
        "                    idx.append(str(item['idx']) + '_' + str(c))\n",
        "                    if len((X_a[-1] + ' ' + X_b[-1]).split()) >= 300:\n",
        "                        del y[-1]\n",
        "                        del X_a[-1]\n",
        "                        del X_b[-1]\n",
        "                        del idx[-1]\n",
        "                        y.append(1)\n",
        "                        X_a.append('Привет')\n",
        "                        X_b.append('Пока')\n",
        "                        idx.append(str(item['idx']) + '_' + str(c))\n",
        "                        print(2)\n",
        "                    dct[idx[-1]] = i\n",
        "    return idx, X_a, X_b, dct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwVwSnJtqAOg"
      },
      "outputs": [],
      "source": [
        "idx_t, X_t_a, X_t_b, dct_ans = get_X_y_for_bert_test('RuCoS/rucos_test.jsonl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4WY8YvfqE_n"
      },
      "outputs": [],
      "source": [
        "df_test = pd.DataFrame({\n",
        "    'id': idx_t,\n",
        "    'sentence1': X_t_a,\n",
        "    'sentence2': X_t_b,\n",
        "    'label': [0 for i in range(len(X_t_a))]\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cje2woxcrKO1"
      },
      "outputs": [],
      "source": [
        "df_test['sentence1'] = df_test['sentence1'].str.lower()\n",
        "df_test['sentence2'] = df_test['sentence2'].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWoWiw6MlPm-"
      },
      "outputs": [],
      "source": [
        "path_to_model = '/content/drive/MyDrive/models/sismetaninrxlm_roberta_large-ru-sentiment-rureviews_lr_5e-06_val_loss_0.70084_ep_4.pt'  \n",
        "\n",
        "path_to_output_file = 'results/output.txt'\n",
        "\n",
        "print(\"Reading test data...\")\n",
        "test_set = CustomDataset(df_val, maxlen, bert_model)\n",
        "test_loader = DataLoader(test_set, batch_size=bs, num_workers=5)\n",
        "\n",
        "model = SentencePairClassifier(bert_model)\n",
        "if torch.cuda.device_count() > 1:  \n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "print()\n",
        "print(\"Loading the weights of the model...\")\n",
        "model.load_state_dict(torch.load(path_to_model))\n",
        "model.to(device)\n",
        "\n",
        "print(\"Predicting on test data...\")\n",
        "idx_all, proba_all = test_prediction(net=model, device=device, dataloader=test_loader, with_labels=True,  # set the with_labels parameter to False if your want to get predictions on a dataset without labels\n",
        "                result_file=path_to_output_file)\n",
        "print()\n",
        "print(\"Predictions are available in : {}\".format(path_to_output_file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_KMjvgrjaeM"
      },
      "outputs": [],
      "source": [
        "dfc1 = {'proba': proba_all, 'labels': df_val['label'].values}\n",
        "df2 = pd.DataFrame(dfc1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNkh5Ak0jo8x"
      },
      "outputs": [],
      "source": [
        "df2.to_csv('train_xlm_300_450.csv') # Сохранение предикта на валидации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "434Fx54L9UjF"
      },
      "outputs": [],
      "source": [
        "dct1 = {'id1': [], 'id2': [], 'proba': []}\n",
        "for i in range(len(idx_all)):\n",
        "    t = idx_all[i].split('_')\n",
        "    dct1['id1'].append(t[0])\n",
        "    dct1['id2'].append(t)\n",
        "    dct1['proba'].append(proba_all[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykvek-WEjYP_"
      },
      "outputs": [],
      "source": [
        "dct1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Wrgqt4rHEJq"
      },
      "outputs": [],
      "source": [
        "lst_ans = []\n",
        "df1 = pd.DataFrame(dct1)\n",
        "for i in df1['id1'].unique():\n",
        "    df2 = df1[df1['id1'] == i]\n",
        "    a = {}\n",
        "    c = 0\n",
        "    for j in range(len(df2)):\n",
        "        if c < df2['proba'].values[j]:\n",
        "            c = df2['proba'].values[j]\n",
        "            a['text'] = dct_ans[df2['id2'].values[j]]['text']\n",
        "    a['idx'] = int(i)\n",
        "    lst_ans.append(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJfcBSrdHLFu"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "class NpEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        else:\n",
        "            return super(NpEncoder, self).default(obj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPOlMg2sHOEf"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('output_02_03_v1.jsonl', 'w') as outfile:\n",
        "    for entry in lst_ans:\n",
        "        json.dump(entry, outfile,  cls=NpEncoder)\n",
        "        outfile.write('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMAYzHUUM1pz"
      },
      "outputs": [],
      "source": [
        "###################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtM-liPqM29s"
      },
      "outputs": [],
      "source": [
        "# ОБЪЕДИНЕНИЕ МОДЕЛЕЙ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsCqNV6cLatC"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2j2Ad-F6MVQO"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/train_v1.csv')\n",
        "trainX, testX, trainY, testY = sklearn.model_selection.train_test_split(train[train.columns[1:-1]].values, train[train.columns[-1]].values, test_size=0.3, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgDeAHnPMbfm"
      },
      "outputs": [],
      "source": [
        "test = pd.read_csv('/content/drive/MyDrive/test_v1.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJCmdHo4MjSs"
      },
      "outputs": [],
      "source": [
        "res = {}\n",
        "res[\"proba\"] = (test[\"proba_xlm_mix_1\"].values + test[\"proba_xlm_mix_2\"].values + test[\"proba_xlm_mix_3\"].values)/ 3 * 0.75 + (test[\"proba_xlm_1\"].values + test[\"proba_xlm_2\"].values + test[\"proba_xlm_3\"].values)/ 3 * 0.25\n",
        "res[\"id1\"] = df1[\"id1\"].values\n",
        "res[\"id2\"] = df1[\"id2\"].values\n",
        "res = pd.DataFrame(res)\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pc9qmco1Mx_O"
      },
      "outputs": [],
      "source": [
        "lst_ans = []\n",
        "df1 = res\n",
        "for i in df1['id1'].unique():\n",
        "    df2 = df1[df1['id1'] == i]\n",
        "    a = {}\n",
        "    c = 0\n",
        "    for j in range(len(df2)):\n",
        "        if c < df2['proba'].values[j]:\n",
        "            c = df2['proba'].values[j]\n",
        "            a['text'] = dct_ans[df2['id2'].values[j]]\n",
        "    a['idx'] = int(i)\n",
        "    lst_ans.append(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtuQPoVdMzgC"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('all_mean_median.jsonl', 'w') as outfile:\n",
        "    for entry in lst_ans:\n",
        "        json.dump(entry, outfile,  cls=NpEncoder)\n",
        "        outfile.write('\\n')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
